{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\development\\llm\\workshop\\env\\lib\\site-packages (4.49.0)\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp311-cp311-win_amd64.whl (204.2 MB)\n",
      "     -------------------------------------- 204.2/204.2 MB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 7.8 MB/s eta 0:00:00\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "     -------------------------------------- 134.6/134.6 kB 8.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "     ---------------------------------------- 6.2/6.2 MB 8.1 MB/s eta 0:00:00\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: colorama in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\development\\llm\\workshop\\env\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Installing collected packages: mpmath, sympy, networkx, jinja2, torch\n",
      "Successfully installed jinja2-3.1.5 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We the people who are not married or divorced , and who are\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Set the device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Define the input text and tokenize it\n",
    "input_text = \"We the people\"\n",
    "\n",
    "for i in range(10):\n",
    "    input_text += \" [MASK]\"\n",
    "    tokenized_text = tokenizer.tokenize(input_text)\n",
    "\n",
    "    # Find the index of the masked token\n",
    "    masked_index = tokenized_text.index('[MASK]')\n",
    "\n",
    "    # Convert the tokenized text to a tensor of token ids\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    # Move the tokens tensor to the CPU\n",
    "    tokens_tensor = tokens_tensor.to(device)\n",
    "\n",
    "    # Generate predictions for the masked token using the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0][0, masked_index].topk(5)\n",
    "\n",
    "    # Convert the predicted token ids to tokens\n",
    "    predicted_token_ids = predictions.indices.tolist()\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)\n",
    "\n",
    "    # Print the predicted tokens\n",
    "    next_token = predicted_tokens[0]\n",
    "    input_text = input_text.replace(\"[MASK]\", next_token)\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your dog looks beautiful, which breed  is it ? \" \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, OpenAIGPTLMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained(\"openai-community/openai-gpt\")\n",
    "\n",
    "input_text = \"Your dog looks beautiful, which breed \"\n",
    "print(input_text, end=\" \")\n",
    "for _ in range(10):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    logits = outputs.logits\n",
    "    predicted_token_id = torch.argmax(logits[0, -1, :]).item()\n",
    "    predicted_word = tokenizer.decode(predicted_token_id)\n",
    "    print(predicted_word, end=\" \")\n",
    "    input_text += \" \" + predicted_word "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Write a poem:\\n The flower was beautiful,\\n\\nthe roses were precious, the butterflies were glorious\\n\\nAnd the stars were lovely.\\n\\nCultural influence: The poem is in a collection called \"Love. The Life\" or \"Love. The Art of Poetry\" and includes an online version (which you can take to your local Post Office)!\\n\\nWhat\\'s your favorite poem to read that you\\'d like to share with people in the world?\\n\\nI am interested in sharing this amazing poem in the post of the week'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"Write a poem:\\n The flower was beautiful,\\n\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a poem:\n",
      " The flower was beautiful,\n",
      "\n",
      "the roses were precious, the butterflies were glorious\n",
      "\n",
      "And the stars were lovely.\n",
      "\n",
      "Cultural influence: The poem is in a collection called \"Love. The Life\" or \"Love. The Art of Poetry\" and includes an online version (which you can take to your local Post Office)!\n",
      "\n",
      "What's your favorite poem to read that you'd like to share with people in the world?\n",
      "\n",
      "I am interested in sharing this amazing poem in the post of the week\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(generated_ids)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
